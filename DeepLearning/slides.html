<!DOCTYPE html>
<html>
  <head>
    <title>Title</title>
    <meta charset="utf-8">
<!-- https://github.com/gnab/remark -->
    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    body {
        font-family: 'Droid Serif';
      }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: 400;
        margin-bottom: 0;
      }
      .remark-slide-content h1 { font-size: 3em; }
      .remark-slide-content h2 { font-size: 2em; }
      .remark-slide-content h3 { font-size: 1.6em; }
      .footnote {
        position: absolute;
        bottom: 3em;
      }
      li p { line-height: 1.25em; }
      .red { color: #fa0000; }
      .large { font-size: 2em; }
      a, a > code {
        color: rgb(249, 38, 114);
        text-decoration: none;
      }
      code {
        background: #e7e8e2;
        border-radius: 5px;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
      .remark-code-line-highlighted     { background-color: #373832; }
      .pull-left {
        float: left;
        width: 47%;
      }
      .pull-right {
        float: right;
        width: 47%;
      }
      .pull-right ~ p {
        clear: both;
      }
      #slideshow .slide .content code {
        font-size: 0.8em;
      }
      #slideshow .slide .content pre code {
        font-size: 0.9em;
        padding: 15px;
      }
      .inverse {
        background: #272822;
        color: #777872;
        text-shadow: 0 0 20px #333;
      }
      .inverse h1, .inverse h2 {
        color: #f3f3f3;
        line-height: 0.8em;
      }

      /* Slide-specific styling */
      #slide-inverse .footnote {
        bottom: 12px;
        left: 20px;
      }
      #slide-how .slides {
        font-size: 0.9em;
        position: absolute;
        top:  151px;
        right: 140px;
      }
      #slide-how .slides h3 {
        margin-top: 0.2em;
      }
      #slide-how .slides .first, #slide-how .slides .second {
        padding: 1px 20px;
        height: 90px;
        width: 120px;
        -moz-box-shadow: 0 0 10px #777;
        -webkit-box-shadow: 0 0 10px #777;
        box-shadow: 0 0 10px #777;
      }
      #slide-how .slides .first {
        background: #fff;
        position: absolute;
        top: 20%;
        left: 20%;
        z-index: 1;
      }
      #slide-how .slides .second {
        position: relative;
        background: #fff;
        z-index: 0;
      }

      /* Two-column layout */
      .left-column {
        color: #777;
        width: 20%;
        height: 92%;
        float: left;
      }
        .left-column h2:last-of-type, .left-column h3:last-child {
          color: #000;
        }
      .right-column {
        width: 75%;
        float: right;
        padding-top: 1em;
      }
    </style>
    </style>
  </head>
  <body>
    <textarea id="source">
      class: center, middle 

      # Variáveis aleatórias e matrizes de covariância
      
      ---
      class: left, middle      
      # Variável Aleatória em 1 dimensão
      
      $$\mathbb{P}(X \le a) = \int_{-\infty}^{a} f(x) dx $$
      com $f$ sendo a função densidade de probabilidade.


      ---
      class: left, middle

      ## Esperança
      
      Também conhecido como _média_ ou _valor esperado_ e denotado por $\mu$.
      
      $$\mathbb{E}(X) = \int_{\mathbb{R}} xf(x) dx$$
      
      --
      
      Em geral, dada $g: \mathbb{R}\to \mathbb{R}$ temos
      
      $$\mathbb{E}(g(X)) = \int_{\mathbb{R}} g(x)f(x) dx$$
      
      ---
      class: left, middle
      
      ## Variância
      
      
      $$\text{Var}(X) = \mathbb{E}( (X - \mu)^2)$$
      
      Mede o quão dispersos estão os dados de $X$: quanto maior a variância, mais longe da média estão os dados.
      
      A raiz quadrada da variância é o desvio-padrão, geralmente denotado por $\sigma$. 
      
      ---
      class: left, middle
      
      ## Variáveis Gaussianas
      
      São caracterizadas pela sua função de densidade
      
      $$f(x) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-\mu)^2}{2\sigma}\right)$$
      
      Uma variável aleatória $X$ com média $\mu$ e variância $\sigma^2$ é denotada por $X\sim N(\mu,\sigma^2)$.
      ---
      class: left, middle
      ## Z-score
      
      Dada uma variável aleatória $X\sim N(\mu,\sigma^2)$, podemos fazer uma transformação 
      
      $$Z = \frac{X- \mu}{\sigma}$$
      
      de modo que $Z \sim  N(0,1)$. 

      ---
      
      # Variável Aleatória em várias dimensões
      
      
      - Temos um vetor $X = (X_1, \ldots, X_n)$ cujas componentes são variáveis aleatórias. 
      - Temos uma função de densidade $f(x_1,\ldots, x_n)$ de modo que 

      $$
      \mathbb{P}(X_1 \le a_1, \ldots, X_n \le a_n) = \int_{-\infty}^{a_1}\ldots\int_{-\infty}^{a_n} f(x_1,\ldots,x_n) dx_n\ldots dx_1
      $$
      
      ---
      
      ## Probabilidade marginal
      
      Surge quando queremos estudar a distribuição geral de uma variável só, "ignorando" as outras. Nesse caso, integramos a variável de interesse no intervalo de interesse, e as outras variáveis são integradas em todo $\mathbb{R}$.
      
      $$\mathbb{P}(X_1 \le a_1)  = \int_{-\infty}^{a_1}\ldots\int_{-\infty}^{\infty} f(x_1,\ldots,x_n) dx_n\ldots dx_1$$
      
      Podemos definir então as densidades marginais
      
      $$f_1(x) =  \int_{-\infty}^{x}\ldots\int_{-\infty}^{\infty} f(x_1,\ldots,x_n) dx_n\ldots dx_1$$
      
      
      ## Esperança
      
      No caso vetorial, o operador esperança atua entrada-a-entrada
      
      $$\mathbb{E}(X) = (\mathbb{E}(X_1), \mathbb{E}(X_2),\ldots, \mathbb{E}(X_n))$$
      
      em que cada esperança é calculada com respeito à distribuição marginal de cada variável.
      
      ---
      
      **Propriedades**
      
      $$\mathbb{E}(c^\top X) = c^\top\mu$$
      
      ## Covariância
      
      Dadas variáveis $X$, $Y$ unidimensionais com médias $\mu_X, \mu_Y$, definimos a covariância de $X$ e $Y$ como 
      $$\text{Cov}(X,Y) = \mathbb{E}( (X - \mu_X)(Y - \mu_Y))$$
      
      Por vezes também é usada a notação $\sigma_{XY}$ para denotar a covariância.
      
      A covariância indica o quanto $X$ e $Y$ estão _linearmente_ relacionadas.
      
      ---
      
      Note que $$\text{Cov}(X,X) = \mathbb{E}( (X - \mu_X)(X - \mu_X)) =  \mathbb{E}((X-\mu_X)^2) =  \text{Var}(X)$$
      
      ---
      
      ## Independência estatística
      
      Duas variáveis $X_1$ e $X_2$ são ditas independentes se for possível escrever a função de densidade como $f(x_1,x_2) = f_1(x_1)f_2(x_2)$
      
      
      
      ### Teorema 
      Se  $X_1$ e $X_2$  são independentes, então 
      $\text{Cov}(X,Y)=0$
      
      ---
      
      ### Contra-exemplo
      
      A recíproca não é verdadeira. Considere $X$ uma variável aleatória com média zero e densidade $f(x)$ sendo uma função par. Defina $Y = X^2$. 
      Nesse caso, claramente $Y$ e $X$ não são independentes, mas 
      $$\text{Cov}(X,Y) = 0 $$ 
      
      
      ---
      
      ## Correlação
      
      É a covariância "normalizada"
      
      $$\text{Corr}(X,Y) = \frac{\text{Cov}(X,Y)}{\sigma_X\cdot\sigma_Y}$$
      
      Também denotada por $\rho_{XY}$.
      
      ## Matriz de Variância-Covariância
      
      - Para calcular a variância de uma variável unidimensional, temos que calcular $\mathbb{E}((X-\mu_X)^2)$
      - Por causa do termo quadrático, temos que adaptar esse cálculo para o caso vetorial. 
      
      Sendo $X = (X_1,\ldots,X_n)$ uma variável aleatória e seja $\mathbb{E}(X) = \mu_X$ seu vetor-médio. Calculamos a variância de $X$ como
      
      $$\Sigma = \mathbb{E}((X-\mu_X)(X-\mu_X)^\top)$$
      
      $$Var(c^\top X) = c^\top\Sigma c$$  
      
      ---
      
      $$\Sigma = \mathbb{E}\begin{pmatrix}  
      (X_1 -\mu_{X_1})(X_1 -\mu_{X_1}) & (X_1 -\mu_{X_1})(X_2 -\mu_{X_2}) & \ldots &  (X_1 -\mu_{X_1})(X_n -\mu_{X_n}) \\
      (X_2 -\mu_{X_2})(X_1 -\mu_{X_1}) & (X_2 -\mu_{X_2})(X_2 -\mu_{X_2}) & \ldots &  (X_2 -\mu_{X_2})(X_n -\mu_{X_n}) \\
      \vdots & \vdots  & \ddots & \vdots \\
      (X_n -\mu_{X_n})(X_1 -\mu_{X_1}) & (X_n -\mu_{X_n})(X_2 -\mu_{X_2}) & \ldots & (X_n -\mu_{X_n})(X_n -\mu_{X_n})
      \end{pmatrix}$$
      
      
      ---
      
      $$\Sigma = \begin{pmatrix}  
      \text{Var}(X_1) & \text{Cov}(X_1,X_2) & \ldots & \text{Cov}(X_1,X_n)\\
      \text{Cov}(X_1,X_2) & \text{Var}(X_2) & \ldots & \text{Cov}(X_2,X_n)\\
      \vdots & \vdots  & \ddots & \vdots \\
      \text{Cov}(X_n,X_1) & \text{Cov}(X_n,X_2) & \ldots & \text{Var}(X_n)\\
      \end{pmatrix}$$
      
      
      ---
      
      ::: {.block}
      
      ### Teorema
      
      A matriz de variância-covariância é definida positiva. 
      ::: 
       
      Dado um vetor $w$ qualquer, note que
      
      $$w^\top \Sigma w = w^\top\mathbb{E}(XX^\top)w = \mathbb{E}((Xw)^\top Xw)$$
       
      
      ---
      
      ## Variância total
      
      $$\text{tr}(\Sigma) = \sum_{i=1}^n \text{Var}(X_i)$$
      
      
      
      ## Variância Generalizada
      
      $$\text{det}(\Sigma)$$
      
      
      
      ## Distância Mahalanobis
      
      Dados vetores $X$ e $Y$ que pertençam a uma distribuição multivariada descrita por uma matriz $\Sigma$, definimos a distância Mahalanobi
      entre eles como 
      
      $$d_{XY}^2 = (X-Y)^\top\Sigma^{-1}(X-Y)$$
      
      Essa é a generalização do Z-score.
      
      
    </textarea>
        <script src="http://gnab.github.io/remark/downloads/remark-latest.min.js" type="text/javascript"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML&delayStartupUntil=configured" type="text/javascript"></script>
    <script type="text/javascript">
      var slideshow = remark.create();

      // Setup MathJax
      MathJax.Hub.Config({
          tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
          }
      });

      MathJax.Hub.Configured();
    </script>
  </body>
</html>
