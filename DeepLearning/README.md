# Princípios de Aprendizagem Profunda

Livro texto: [The Principles of Deep Learning Theory](https://arxiv.org/abs/2106.10165) (Daniel A. Roberts, Sho Yaida, Boris Hanin)

[Minhas apresentações](./minhas-apresentacoes.md)

## Conteúdos online

- [Alguns conceitos teóricos de base relacionados às redes neurais](https://homepages.dcc.ufmg.br/~gabriel/pestov/)
- [Curso IME](https://www.ime.usp.br/~leonardi/curso_pestov/)
- [Elementos da Teoria de Aprendizagem de Máquina Supervisionada - Vladimir G. Pestov](https://impa.br/wp-content/uploads/2022/03/32CBM07_eBook.pdf)
- [CSE 254: Seminar on Learning Algorithms](https://cseweb.ucsd.edu/~dasgupta/254-deep/)
	- [Weighted sums of random kitchen sinks](https://papers.nips.cc/paper_files/paper/2008/file/0efe32849d230d7f53049ddc4a4b0c60-Paper.pdf)
	- [Learning Deep Energy Models](https://ai.stanford.edu/~ang/papers/icml11-DeepEnergyModels.pdf)
- [Sanjoy Dasgupta - Teaching](https://cseweb.ucsd.edu/~dasgupta/courses.html)
- [CS229T/STAT231: Statistical Learning Theory (Winter 2016) - Percy Liang](https://web.stanford.edu/class/cs229t/notes.pdf)
- [Andoni/Panigrahy/Valiant/Zhang, 2014: Learning Polynomials with Neural Networks](https://theory.stanford.edu/~valiant/papers/andoni14.pdf)
- [Schmidhuber, 2014: Deep Learning in Neural Networks: An Overview](https://arxiv.org/pdf/1404.7828v4)
- [University of Cambridge -  Teaching - Mathematics of Machine Learning](https://www.statslab.cam.ac.uk/~rds37/machine_learning.html)
- [Cybenko’s Theorem and the capability of a neural network as function approximator](https://www.mathematik.uni-wuerzburg.de/fileadmin/10040900/2019/Seminar__Artificial_Neural_Network__24_9__.pdf)
- [Introduction to Online Convex Optimization](https://arxiv.org/pdf/1909.05207)
- [ECE 598MR: Statistical Learning Theory (Fall 2014)](https://maxim.ece.illinois.edu/teaching/fall14/schedule.html)
- [Sasha Rakhlin - Professor @ MIT | Statistics and Data Science Center, IDSS](https://www.mit.edu/~rakhlin/)
- [*Benign overfitting in overparametrized models*](https://www.cambridge.org/core/services/aop-cambridge-core/content/view/7BCB89D860CEDDD5726088FAD64F2A5A/S0962492921000027a.pdf/deep-learning-a-statistical-viewpoint.pdf)
- [Mathematics of Machine Learning](https://ocw.mit.edu/courses/18-657-mathematics-of-machine-learning-fall-2015/pages/readings/)
- [High-Dimensional Probability](https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.html#)
- [Train faster, generalize better: Stability of stochastic gradient descent](https://proceedings.mlr.press/v48/hardt16.pdf)


# Neural Nets

- [Neural Nets from Scratch in Julia [PART 1]: Introduction](https://www.youtube.com/watch?v=Kybn21KA96g)

## Artigos lidos

1) Artigo de Lin, Tegmark e Rolnik [“Why does deep and cheap learning work so well?”](https://arxiv.org/abs/1608.08225)
2) Artigo de Tegmark e Rolnick [“The power of deeper networks for expressing natural functions”](https://arxiv.org/abs/1705.05502)
3) Slides da [Teoria de Vapnik–Chervonenkis](https://drive.google.com/file/d/1YbOwoq9TZwCQkOy603nyE-lUVIk4ZRsz/view)
4) [Slides Sontag](https://drive.google.com/file/d/1V92dqLg0L3pz5awyvKIavv_FlqtN3Wuz/view) baseados em Eduardo Sontag [Feedforward Nets for Interpolation and Classification](https://www.sciencedirect.com/science/article/pii/002200009290039L?via%3Dihub) [pdf](./referencias/sontag_nets.pdf)
5) Gallant & White [There exists a NN that doesnt make avoidable mistakes](./referencias/gallant_white_NN_avoidable_mistakes.pdf)
6) [Convergência e complexidade do algoritmo de treinamento de perceptron](https://drive.google.com/file/d/1q3B4ClM9YHQkrcomj5_3bwkGYCydjCw9/view)
7) [Train faster, generalize better: Stability of stochastic gradient descent](https://arxiv.org/pdf/1509.01240)

## Artigos para ler

1) Owen Davis and Mohammad Motamed [Approximation Power of Deep Neural Networks - An explanatory mathematical survey](https://arxiv.org/pdf/2207.09511)